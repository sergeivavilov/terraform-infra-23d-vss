.
├── README.md
├── all_project_contents.txt
├── eks-module
│   ├── aws_auth.tf
│   ├── eks.tf
│   ├── node-group.tf
│   ├── variables.tf
│   └── workers.tf
├── nginx-deployment.yaml
├── output.txt
├── rds-mysql-module
│   ├── main.tf
│   └── variables.tf
└── root
    └── main-eks-root
        ├── homework-project.vss.auto.tfvars
        ├── main.tf
        ├── mainvars.tf
        └── providers.tf

5 directories, 15 files
.github/workflows/terraform-deploy.yaml

name: Terraform Deployment Workflow

on:
  # Triggers the workflow on push or pull request events but only for the "main" branch
  push:
    branches: ["*"]

permissions:
  id-token: write # This is required for requesting the JWT
  contents: read  # This is required for actions/checkout

jobs:
  deploy:
    runs-on: ubuntu-latest
    # Select environment based on branch pushed
    environment: ${{ (github.ref == 'refs/heads/main' && 'dev') || (github.ref == 'refs/heads/prod' && 'production') }}

    steps:
      - name: AWS Loggin
        uses: aws-actions/configure-aws-credentials@v3
        with:
          role-to-assume: ${{ vars.AWS_IAM_ROLE }}
          role-session-name: githubactionsbot 
          aws-region: "us-east-1"

      - name: clone repo 
        uses: actions/checkout@v4

      - name: initialize Terraform
        run: terraform init 
        working-directory: ./root/main-eks-root

      - name: Plan execution
        run: terraform plan -var-file="homework-project.vss.auto.tfvars"
        working-directory: ./root/main-eks-root

      # - name: execute terraform
      #   run: terraform apply -var-file="homework-project.notyouraverageusername.auto.tfvars" -auto-approve
      #   working-directory: ./root/main-eks-root
      
      # - name: destroy infrastructure
      #   run: terraform destroy -auto-approve
      #   working-directory: ./root/main-eks-root./eks-module/workers.tf

data "aws_ssm_parameter" "eks_ami_id" {
  name = "/aws/service/eks/optimized-ami/1.29/amazon-linux-2/recommended/image_id"
}

resource "aws_launch_template" "eks_workers" {
  name_prefix   = "${var.name}-eks-worker-nodes"
  image_id      = data.aws_ssm_parameter.eks_ami_id.value
  instance_type = var.instance_type
  depends_on = [null_resource.update_aws_auth,# kubernetes_config_map.aws_auth,
    aws_iam_role_policy_attachment.example-AmazonEKSWorkerNodePolicy,
    aws_iam_role_policy_attachment.example-AmazonEKS_CNI_Policy,
  aws_iam_role_policy_attachment.example-AmazonEC2ContainerRegistryReadOnly]

  vpc_security_group_ids = [aws_security_group.eks_worker_sg.id]
  user_data = base64encode(<<-EOF
#!/bin/bash
set -o xtrace
/etc/eks/bootstrap.sh ${var.name}
EOF
  )
}

resource "aws_security_group" "eks_worker_sg" {
  name        = "eks-worker-sg"
  description = "Security group for EKS worker nodes"

  vpc_id = var.vpc_id
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}
output "worker_sg" {
  value = aws_security_group.eks_worker_sg
}
./eks-module/node-group.tf
resource "aws_eks_node_group" "project-x-node-group" {
  cluster_name    = var.name
  node_group_name = "${var.name}-node-group"
  node_role_arn   = aws_iam_role.example.arn
  subnet_ids      = var.subnet_ids

  scaling_config {
    desired_size = var.capacity[0]
    max_size     = var.capacity[1]
    min_size     = var.capacity[2]
  }

  launch_template {
    version = aws_launch_template.eks_workers.latest_version
    id      = aws_launch_template.eks_workers.id
  }

  update_config {
    max_unavailable = 1
  }

  # Ensure that IAM Role permissions are created before and deleted after EKS Node Group handling.
  depends_on = [
    aws_iam_role_policy_attachment.example-AmazonEKSWorkerNodePolicy,
    aws_iam_role_policy_attachment.example-AmazonEKS_CNI_Policy,
    aws_iam_role_policy_attachment.example-AmazonEC2ContainerRegistryReadOnly,
    null_resource.update_aws_auth
    #kubernetes_config_map.aws_auth
  ]
}


resource "aws_iam_role" "example" {
  name = "eks-node-group-example"

  assume_role_policy = jsonencode({
    Statement = [{
      Action = "sts:AssumeRole"
      Effect = "Allow"
      Principal = {
        Service = "ec2.amazonaws.com"
      }
    }]
    Version = "2012-10-17"
  })
}

resource "aws_iam_role_policy_attachment" "example-AmazonEKSWorkerNodePolicy" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"
  role       = aws_iam_role.example.name
}

resource "aws_iam_role_policy_attachment" "example-AmazonEKS_CNI_Policy" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"
  role       = aws_iam_role.example.name
}

resource "aws_iam_role_policy_attachment" "example-AmazonEC2ContainerRegistryReadOnly" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
  role       = aws_iam_role.example.name
}


./eks-module/aws_auth.tf
# this script fixes error: configmaps "aws-auth" is forbidden: User "system:anonymous" cannot get resource "configmaps"

# return current region details
data "aws_region" "current" {}

# return current user details along with account details
data "aws_caller_identity" "current" {}

# current account number
locals {
  account = data.aws_caller_identity.current.account_id
}

resource "null_resource" "update_aws_auth" {
  depends_on = [aws_eks_cluster.cluster]

# code below allows nodes to runn bootstrap script, and allows custom set of users even crossaccount to communicate to our cluster
  provisioner "local-exec" {
    command = <<-EOT
    sleep 50
    aws eks update-kubeconfig --name ${var.name} --region ${data.aws_region.current.name}
    cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ConfigMap
metadata:
  name: aws-auth
  namespace: kube-system
data:
  mapRoles: |
    - groups:
      - system:bootstrappers
      - system:nodes
      rolearn: ${aws_iam_role.example.arn}
      username: system:node:{{EC2PrivateDNSName}}
    - groups:
      - system:masters
      rolearn: arn:aws:iam::730335359268:user/vss1
      username: adminRoleUser
    - groups:
      - system:masters
      rolearn: arn:aws:iam::730335359268:role/GitHubActionsCICDrole
      username: CICDrole
EOF
    EOT
  }
}
./eks-module/variables.tf
variable "name" {

}
variable "CIDR" {

}
variable "instance_type" {

}
variable "cluster_tag" {

}
variable "vpc_id" {

}
variable "subnet_ids" {

}
variable "alternative_instance_type" {

}
variable "capacity" {

}
variable "spot_allocation_strategy" {

}
variable "availability-zone" {

}
variable "subnet_ip_range_1" {

}
variable "subnet_ip_range_2" {

}

./eks-module/eks.tf
resource "aws_eks_cluster" "cluster" {
  name     = var.name
  role_arn = aws_iam_role.eks_cluster_role.arn
  version  = "1.29"

  vpc_config {
    subnet_ids         = var.subnet_ids
    security_group_ids = [aws_security_group.eks_cluster_sg.id]
  }
  kubernetes_network_config {
    service_ipv4_cidr = var.CIDR
  }

  # make sure to add this so the cluster creates after the role exists
  depends_on = [
    aws_iam_role_policy_attachment.eks_cluster_role-AmazonEKSClusterPolicy
  ]

  tags = {
    Name = var.cluster_tag
  }
}


# trust policy for the role
data "aws_iam_policy_document" "assume_role" {
  statement {
    effect = "Allow"

    principals {
      type        = "Service"
      identifiers = ["eks.amazonaws.com"]
    }

    actions = ["sts:AssumeRole"]
  }
}

# create IAM role
resource "aws_iam_role" "eks_cluster_role" {
  name               = "${var.name}-eks-iam-role"
  assume_role_policy = data.aws_iam_policy_document.assume_role.json
}

resource "aws_iam_role_policy_attachment" "eks_cluster_role-AmazonEKSClusterPolicy" {
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSClusterPolicy"
  role       = aws_iam_role.eks_cluster_role.name
}


# security group for cluster
resource "aws_security_group" "eks_cluster_sg" {
  name        = "EKS updated group"
  description = "Security group for all inbound and outbound traffic for EKS"
  vpc_id      = var.vpc_id
}
resource "aws_vpc_security_group_ingress_rule" "allow_worker_traffic" {
  security_group_id = aws_security_group.eks_cluster_sg.id
  from_port   = 0
  ip_protocol = "tcp"
  to_port     = 65535
  referenced_security_group_id = aws_security_group.eks_worker_sg.id
}
resource "aws_vpc_security_group_ingress_rule" "allow_self_traffic" {
  security_group_id = aws_security_group.eks_cluster_sg.id
  from_port   = 0
  ip_protocol = "tcp"
  to_port     = 65535
  referenced_security_group_id = aws_security_group.eks_cluster_sg.id
} 

# CNI add-on that is missing from EKS configuration
resource "aws_eks_addon" "cni_pluggin" {
  cluster_name  = aws_eks_cluster.cluster.name
  addon_name    = "vpc-cni"
  addon_version = "v1.18.1-eksbuild.1"
  depends_on    = [aws_eks_cluster.cluster]
}
resource "aws_eks_addon" "kube_proxy" {
  cluster_name  = aws_eks_cluster.cluster.name
  addon_name    = "kube-proxy"
  addon_version = "v1.29.1-eksbuild.2" 
  depends_on    = [aws_eks_cluster.cluster]
}

output "endpoint" {
  value = aws_eks_cluster.cluster.endpoint
}

output "kubeconfig-certificate-authority-data" {
  value = aws_eks_cluster.cluster.certificate_authority
}

./rds-mysql-module/main.tf
resource "aws_db_instance" "rds_instance" {
  identifier              = var.db_instance_identifier
  allocated_storage       = var.db_allocated_storage
  db_name                 = var.db_name
  engine                  = var.engine
  engine_version          = var.engine_version
  instance_class          = var.instance_class
  username                = var.db_username
  backup_retention_period = var.backup_retention_period
  skip_final_snapshot         = true
  vpc_security_group_ids      = [aws_security_group.rds-sg.id]
  manage_master_user_password = true
  db_subnet_group_name        = aws_db_subnet_group.db_subnet_group.name
  auto_minor_version_upgrade  = true
}

resource "aws_security_group" "rds-sg" {
  name   = "rds_sg"
  vpc_id = var.vpc_id

  ingress {
    from_port       = 5432
    to_port         = 5432
    protocol        = "tcp"
    security_groups = [var.worker_sg]

  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"] # All outbound traffic allowed
  }
  tags = {
    Name = "rds_sg"
  }
}

resource "aws_db_subnet_group" "db_subnet_group" {
  name       = var.db_subnet_group_name
  subnet_ids = var.subnet_ids
}

# this secret will hold all connection info except password
# password will be auto generated by rds and stored in separate secret
resource "aws_secretsmanager_secret" "rds_login" {
  name        = "RDS_connection_final"
  description = "RDS credentials"
  recovery_window_in_days = 0 # will delete secret right away, probably not a good idea for production
}

resource "aws_secretsmanager_secret_version" "RDS_connection_points" {
  secret_id = aws_secretsmanager_secret.rds_login.id
  secret_string = jsonencode({
    DB_USERNAME = var.db_username
    DB_ENDPOINT = aws_db_instance.rds_instance.endpoint
    DB_NAME     = var.db_name
  })
}
./rds-mysql-module/variables.tf
variable "db_instance_identifier" {

}
variable "db_allocated_storage" {

}
variable "db_name" {

}
variable "engine" {

}
variable "engine_version" {

}
variable "instance_class" {

}
variable "db_username" {

}
variable "backup_retention_period" {

}
variable "vpc_id" {

}
variable "db_subnet_group_name" {

}
variable "subnet_ids" {

}
variable "worker_sg" {

}
./root/main-eks-root/main.tf
# module "project-x-eks-cluster" {
#   source                    = "../../eks-module"
#   name                      = var.name
#   CIDR                      = var.CIDR
#   instance_type             = var.instance_type
#   cluster_tag               = var.cluster_tag
#   vpc_id                    = var.vpc_id
#   subnet_ids                = var.subnet_ids
#   alternative_instance_type = var.alternative_instance_type
#   capacity                  = var.capacity # capacity list: desired, max, min
#   spot_allocation_strategy  = var.spot_allocation_strategy
#   availability-zone         = var.availability-zone
#   subnet_ip_range_1         = var.subnet_ip_range_1
#   subnet_ip_range_2         = var.subnet_ip_range_2
# }


# module "rds-mysql" {
#   source                  = "../../rds-mysql-module"
#   backup_retention_period = var.backup_retention_period
#   subnet_ids              = var.subnet_ids
#   engine                  = var.engine
#   engine_version          = var.engine_version
#   vpc_id                  = var.vpc_id
#   worker_sg               = module.project-x-eks-cluster.worker_sg.id
#   db_name                 = var.db_name
#   instance_class          = var.instance_class
#   db_username             = var.db_username
#   db_instance_identifier  = var.db_instance_identifier
#   db_allocated_storage    = var.db_allocated_storage
#   db_subnet_group_name    = var.db_subnet_group_name
# }
./root/main-eks-root/providers.tf
# Provider
provider "aws" {
  region = "us-east-1"
}

terraform {
  backend "s3" {
    bucket = "githubactions-terrafrom-task"
    key    = "terraform.tfstate"
    region = "us-east-1"
  }
}

# this provider required for configmap creation
terraform {
  required_providers {
    kubernetes = {
      source = "hashicorp/kubernetes"
    }
  }
}

provider "kubernetes" {
  host                   = module.project-x-eks-cluster.endpoint
  cluster_ca_certificate = base64decode(module.project-x-eks-cluster.kubeconfig-certificate-authority-data[0].data)
}

./root/main-eks-root/mainvars.tf
variable "name" {

}
variable "CIDR" {

}
variable "instance_type" {

}
variable "cluster_tag" {

}
variable "vpc_id" {

}
variable "subnet_ids" {

}
variable "alternative_instance_type" {

}
variable "capacity" {

}
variable "spot_allocation_strategy" {

}
variable "availability-zone" {

}
variable "subnet_ip_range_1" {

}

variable "subnet_ip_range_2" {

}


# variables for RDS database

variable "db_instance_identifier" {

}
variable "db_allocated_storage" {

}
variable "db_name" {

}
variable "engine" {

}
variable "engine_version" {

}
variable "instance_class" {

}
variable "db_username" {

}
variable "backup_retention_period" {

}

variable "db_subnet_group_name" {

}
